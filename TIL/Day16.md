# Today I Learned

> ## 앙상블[Ensemble]Learning

- 앙상블 유형 : 보팅(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)등의 기
  법이 있음
- 배깅(Bagging) : 랜덤 포레스트(Random Forest) 알고리즘
- 부스팅(Boosting) : 에이다 부스팅, 그래이던트 부스팅, XGBoost, LightBGM등
- 넒은 의미 : 서로 다른 알고리즘들을 결합



> ## 분류(Classfication)

- 단일 모델의 약점을 다수의 모델을 결합하여 보완함

- 뛰어난 성능들만 구성하는 것만이 좋은 것은 아니다.

- 랜던포레스트, 뛰어난 부스팅 알고리즘들은 모두 결정 트리 알고리즘을 기반으로 함

- 결정트리의 과적합 문제를 다수의 분류기를 결합해서 보완, 직관적인 분류기준은 강화



- 보팅 유형

  - 하드보팅, 소프트보팅

    보팅 방법에는 하드 보팅과 소프트 보팅 방식이 있다.

    하드 보팅은 **예측한 결괏값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 방식**이다. 다수결의 원칙이라고 생각하면 된다.

    ![img](https://postfiles.pstatic.net/MjAyMTA4MjZfNTkg/MDAxNjI5OTUyNjI0Mzg4.YfSAKGbFxMiULfa1EVxIkPs8lp9wXkunLIY6fi-ufu8g.ESDz56hTEA7C6RcMIv8Hcf0cm4yUqNt0Zgfjzx4dcTUg.PNG.fbfbf1/image.png?type=w773)

    위의 그림은 하드 보팅을 표현한 것이다. 분류기 1,2, 4는 1로 레이블 값을 예측하고 분류기 3만 2번 레이블로 예측한다. 그럼 다수결의 원칙에 따라서 최종 예측은 1이 된다.

    

    

    소프트 보팅은 분류기들의 **레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로** 선정한다.

    ![img](https://postfiles.pstatic.net/MjAyMTA4MjZfMjUg/MDAxNjI5OTUyNzg1MTg3.Ipq313-jpC-eZ8d4vHU_dqNJ6x4mlODMCJHSUGkn8IQg.fShVu2w1P_lmgN3xQV2hOpGQGO22hvvkzyMydl3yWsog.PNG.fbfbf1/image.png?type=w773)

    소프트 보팅을 그림으로 표현한 것이다. 각 분류기의 레이블 값 예측 확률을 평균 내어 최종 결정을 한다.

    Class 1의 확률 평균은 (0.7 + 0.2 + 0.8 + 0.9) / 4 = 0.65

    Class 2의 확률 평균은 (0.3 + 0.8 + 0.2 + 0.1) / 4 = 0.35이기에 최종 예측은 Class 1이 된다.

    

    보통은 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용된다.

    

## 랜덤 포레스트란

(**“무작위 숲”**이라는 이름처럼) **랜덤 포레스트는** **훈련을 통해 구성해놓은 다수의 나무들로부터 분류 결과를 취합해서 결론을 얻는**, 일종의 **인기 투표**(?) 같은 거다.

아래 그림처럼.

![img](https://hleecaster.com/wp-content/uploads/2020/01/rf01.png)

물론 몇몇의 나무들이 오버피팅을 보일 순 있지만 다수의 나무를 기반으로 예측하기 때문에 그 영향력이 줄어들게 된어 좋은 일반화 성능을 보인다.

이렇게 좋은 성능을 얻기 위해 다수의 학습 알고리즘을 사용하는 걸 **앙상블(ensemble) 학습법**이라고 부른다.

일단 랜덤 포레스트에서 각 나무들을 어떻게 생성하는지 알아야 한다. 결론부터 얘기하면… 배깅(bagging)이라는 프로세스를 통해 나무를 만든다.





## 배깅(Bagging)

학습 데이터 세트에 총 1000개의 행이 있다고 해보자. 그러면 임의로 100개씩 행을 선택해서 의사결정 트리를 만드는 게 **배깅(bagging)**이다. 물론 이런 식으로 트리를 만들면 모두 다르겠지만 그래도 어쨌거나 학습 데이터의 일부를 기반으로 생성했다는 게 중요하다.

그리고 이 때 **중복을 허용**해야 한다는 걸 기억하자.

1000개의 행이 있는 가방(bag)에서 임의로 100개 뽑아 첫 번째 트리를 만들고 그 100개의 행은 가방에 도로 집어 넣는다. 그리고 다시 1000개의 행에서 또 임의로 100개를 뽑아 두 번째 트리를 만든 후 다시 가방에 집어 넣고 뭐 이런 식.

#### Bagging Features

여기에 **트리를 만들 때 사용될 속성(feature)들을 제한**함으로써 각 나무들에 다양성을 줘야 한다.

원래는 트리를 만들 때 모든 속성들을 살펴보고 정보 획득량이 가장 많은 속성을 선택해서 그걸 기준으로 데이터를 분할했다. 그러나 이제는 **각 분할에서 전체 속성들 중 일부만 고려하여 트리를 작성하도록** 하는 전략이다.

예를 들면 총 25개의 속성이 있는데, 그 중 5개의 속성만 뽑아서 살펴본 후 그 중 정보 획득량이 가장 높은 걸 기준으로 데이터를 분할하는 거다. 그 다음 단계에서도 다시 임의로 5개만 선택해서 살펴보고… 뭐 이런 식.

그렇다면 몇개씩 속성을 뽑는 게 좋을까. 위 예처럼 총 속성이 25개면 5개, 즉 **전체 속성 개수의 제곱근만큼 선택**하는 게 가장 좋다고, 경험적으로 그렇게 나타난다고 한다. (일종의 a rule of thumb이다.)

이제 서로 다른 트리를 만들 수 있게 되었으니 이것들을 모아 ‘숲’을 이루도록 하면 되는 거다.